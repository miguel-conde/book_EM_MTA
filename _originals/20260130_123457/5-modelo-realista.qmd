# Incorporando features de HCP (modelo realista)

Este capÃ­tulo es **el salto a producciÃ³n**.\
Pasamos de â€œun $\lambda_h$ libre por HCPâ€ a un modelo **paramÃ©trico, generalizable y regularizado**.

## 1ï¸âƒ£ Por quÃ© necesitamos covariables (motivaciÃ³n)

En el CapÃ­tulo 4 vimos un problema clave:

-   HCPs â€œexclusivosâ€ de bricks grandes pueden absorber demasiada venta

-   El modelo es **dÃ©bilmente identificable**

-   No generaliza a HCPs nuevos

ğŸ‘‰ SoluciÃ³n: **atar la productividad a informaciÃ³n observable**.

## 2ï¸âƒ£ Nueva definiciÃ³n de productividad

Definimos:

$$
\boxed{\lambda_h = \exp(X_h \beta)}
$$

donde:

-   $X_h \in \mathbb{R}^p$: features del HCP\
    (especialidad, seniority, nÂº visitas, potencial, etc.)

-   $\beta \in \mathbb{R}^p$: coeficientes globales

ğŸ“Œ InterpretaciÃ³n:

-   Dos HCPs con mismos $X$ tienen misma productividad esperada

-   El modelo **comparte fuerza estadÃ­stica**

## 3ï¸âƒ£ Modelo completo con covariables (Poisson)

### ğŸ”¹ Generativo

$$
z_{h,b} \sim \text{Poisson}(\exp(X_h\beta), w_{h,b})
$$

$$
y_b = \sum_{h\in b} z_{h,b}
$$

## 4ï¸âƒ£ EM + GLM: cÃ³mo encajan

### ğŸ” E-step (igual que antes)

Con $\lambda_h^{(t)} = \exp(X_h\beta^{(t)})$:

$$
\boxed{
\mathbb{E}\big[z_{h,b}\mid y_b,\beta^{(t)}\big]
= y_b\cdot
\frac{\exp(X_h\beta^{(t)})\, w_{h,b}}
{\sum_{h'\in b}\exp(X_{h'}\beta^{(t)})\, w_{h',b}}
}
$$

ğŸ“Œ **Nada cambia conceptualmente**.

### ğŸ”„ M-step (ahora es un GLM)

Definimos los â€œpseudo-targetsâ€:

$$
    ilde y_h = \sum_b \mathbb{E}[z_{h,b}]
$$

y la exposiciÃ³n total:

$$
    ilde w_h = \sum_b w_{h,b}
$$

Entonces el M-step es:

$$
\boxed{
\max_{\beta} \sum_h \left[ \tilde y_h \log \lambda_h - \lambda_h \tilde w_h \right]
\quad
    \text{con }\lambda_h = \exp(X_h\beta)
}
$$

ğŸ‘‰ Esto es exactamente una **Poisson regression con offset**:

-   Response: $\tilde y_h$

-   Offset: $\log(\tilde w_h)$

ğŸ“Œ Se resuelve con IRLS estÃ¡ndar.

## 5ï¸âƒ£ Caso euros (Gamma + covariables)

AnÃ¡logo:

$$
z_{h,b} \sim \text{Gamma}(k, \theta_h w_{h,b})
\quad
    \text{con }\theta_h = \exp(X_h\beta)
$$

-   E-step: reparto proporcional a $\theta_h w_{h,b}$

-   M-step:

    -   $\beta$: GLM Gamma con link log

    -   $k$: fijo o estimado con Newton

ğŸ“Œ Misma lÃ³gica, distinta distribuciÃ³n.

## 6ï¸âƒ£ RegularizaciÃ³n (crÃ­tica en la prÃ¡ctica)

Sin regularizaciÃ³n:

-   $\beta$ puede explotar

-   Especialmente si hay features raras o colineales

### ğŸ”¹ PenalizaciÃ³n tÃ­pica

$$
\beta \sim \mathcal{N}(0,\sigma^2 I)
$$

Equivalente a:

-   Ridge (L2)

-   MAP en versiÃ³n bayesiana

ğŸ“Œ Muy recomendable en producciÃ³n.

## 7ï¸âƒ£ Identificabilidad (quÃ© se arregla y quÃ© no)

### âœ”ï¸ Lo que mejora

-   HCPs exclusivos ya no â€œabsorbenâ€ arbitrariamente

-   Productividades estÃ¡n ancladas a $X_h$

-   Generaliza a HCPs nuevos

### âŒ Lo que sigue siendo cierto

-   Attribution â‰  causalidad

-   Si dos HCPs tienen $X$ casi idÃ©ntico y siempre co-ocurren, no se distinguen

## 8ï¸âƒ£ Â¿QuÃ© pasa cuando un HCP estÃ¡ en muchos bricks?

Esto es **bueno**, no malo.

-   Aparece en muchas ecuaciones:

    $$
    y_b \approx \sum_h \exp(X_h\beta)\, w_{h,b}
    $$

-   El modelo **lo usa como ancla**

-   Reduce varianza de $\beta$

ğŸ“Œ En la prÃ¡ctica:

-   HCPs â€œcentralesâ€ estabilizan el modelo

-   HCPs raros se benefician del pooling

## 9ï¸âƒ£ âœ”ï¸ SALIDA â€” Modelo listo para datos reales

### Modelo final (unidades)

$$
\boxed{
\begin{aligned}
z_{h,b} &\sim \text{Poisson}(\exp(X_h\beta), w_{h,b}) \\
y_b &= \sum_{h\in b} z_{h,b}
\end{aligned}
}
$$

con:

-   EM para inferencia

-   GLM en el M-step

-   RegularizaciÃ³n opcional

## ğŸ”š Lectura de negocio (muy importante)

> â€œEstamos estimando un **modelo probabilÃ­stico de generaciÃ³n de ventas**,\
> usando informaciÃ³n observable del HCP,\
> y la attribution es la **mejor explicaciÃ³n consistente** con ese modelo.â€