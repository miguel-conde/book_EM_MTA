# El problema como modelo de variables latentes

## ReformulaciÃ³n del business problem (de negocio â†’ estadÃ­stica)

### ğŸ”¹ FormulaciÃ³n de negocio

-   Tenemos **HCPs** distribuidos en **bricks**

-   Cada brick reporta **ventas totales agregadas**

-   No observamos ventas por HCP

-   Queremos **atribuir** esas ventas a HCPs individuales

-   Disponemos de **features por HCP** (especialidad, visitas, demografÃ­aâ€¦)

### ğŸ”¹ ReformulaciÃ³n estadÃ­stica

Esto no es un problema de predicciÃ³n directa, sino de **inferir una descomposiciÃ³n no observada**.

> â€œLas ventas de cada brick son la suma de contribuciones latentes de los HCPs que lo componen.â€

Formalmente:

-   Existe un **proceso generativo no observado**:

    -   Cada HCP genera ventas

    -   Parte de esas ventas se manifiestan en cada brick

-   Solo observamos el **resultado agregado**

ğŸ‘‰ Esto es exactamente un problema de **variables latentes + datos incompletos**.

## 2ï¸âƒ£ QuÃ© estÃ¡ observado y quÃ© es latente

### ğŸ” Observado (datos)

-   ( y_b ): ventas totales del brick ( b )

-   ( X_h ): covariables del HCP ( h )

-   ( M\_{h,b} \\in {0,1} ): pertenencia HCPâ€“brick\
    (o peso si la pertenencia no es binaria)

### ğŸ‘» Latente (no observado)

-   ( z\_{h,b} ): contribuciÃ³n de ventas del HCP ( h ) al brick ( b )

con la restricciÃ³n:\
\[\
\\sum\_{h \\in b} z\_{h,b} = y_b\
\]

ğŸ“Œ Este ( z\_{h,b} ) es **la variable oculta clave**.

## 3ï¸âƒ£ Por quÃ© esto NO es una regresiÃ³n directa

### âŒ RegresiÃ³n naÃ¯ve

PodrÃ­as intentar algo como:

\[\
y_b = \\sum\_{h \\in b} f(X_h)\
\]

Problemas:

-   No puedes identificar contribuciones individuales

-   No hay likelihood bien definida

-   No hay forma natural de repartir ( y_b )

-   Mal planteado para inferencia

### âŒ SHAP / Attribution post-hoc

SHAP asume:

-   PredicciÃ³n individual

-   FunciÃ³n conocida

-   Observaciones independientes

AquÃ­:

-   El target es **agregado**

-   La unidad de decisiÃ³n (HCP) **no tiene label**

-   El problema es **estructuralmente inverso**

ğŸ‘‰ SHAP responde *â€œpor quÃ© el modelo predijoâ€*,\
no *â€œcÃ³mo se generaron los datosâ€*.

## 4ï¸âƒ£ Por quÃ© EM es el algoritmo natural

### ğŸ”‘ ObservaciÃ³n clave

Si conociÃ©ramos ( z\_{h,b} ):

-   PodrÃ­amos estimar fÃ¡cilmente productividad de HCPs

-   PodrÃ­amos ajustar un GLM estÃ¡ndar

Si conociÃ©ramos la productividad de HCPs:

-   PodrÃ­amos repartir las ventas esperadas entre ellos

ğŸ‘‰ **EM alterna exactamente entre estas dos cosas**.

### ğŸ§  EM en una frase (en este contexto)

> EM alterna entre **atribuir probabilÃ­sticamente ventas a HCPs** (E-step)\
> y **actualizar el modelo de productividad de HCPs** (M-step)

## 5ï¸âƒ£ RelaciÃ³n con modelos conocidos

### ğŸ”¹ Mixture Models

Cada venta del brick proviene de un **â€œcomponenteâ€ HCP**:

\[\
p(\\text{venta} \\mid b) = \\sum\_{h \\in b} \\pi\_{h,b} , p_h\
\]

-   HCPs = componentes

-   ( \\pi\_{h,b} ) = peso del HCP en el brick

-   ( z\_{h,b} ) = responsabilidades

ğŸ“Œ Es un **mixture model con labels parcialmente observados**.

### ğŸ”¹ Ecological Inference

ClÃ¡sico problema:

-   Datos agregados

-   Inferencia a nivel individual

-   MÃºltiples soluciones posibles

EM:

-   Impone un **modelo generativo**

-   Regulariza vÃ­a covariables

-   Evita soluciones arbitrarias

### ğŸ”¹ Credit Attribution

Muy similar a:

-   Multi-touch attribution en marketing

-   Reparto de revenue entre canales

Diferencia clave:

-   AquÃ­ **las â€œtouchesâ€ no se observan**

-   Solo vemos el revenue final agregado

## 6ï¸âƒ£ DefiniciÃ³n clara del modelo generativo (SALIDA)

Este es el **modelo conceptual que usaremos**:

### ğŸ”¹ Nivel HCP

Cada HCP tiene una **productividad latente**:\
\[\
\\theta_h = f(X_h; \\beta)\
\]

Ejemplos:

-   Unidades â†’ ( \\theta_h = \\lambda_h ) (Poisson)

-   Euros â†’ media Gamma / Lognormal

### ğŸ”¹ Nivel Brick

Las ventas del brick se generan como:\
\[\
y_b = \\sum\_{h \\in b} z\_{h,b}\
\]

con:\
\[\
z\_{h,b} \\sim \\text{DistribuciÃ³n}(\\theta_h, w\_{h,b})\
\]

donde:

-   ( w\_{h,b} ): exposiciÃ³n / peso del HCP en el brick\
    (visitas, tiempo, overlap geogrÃ¡ficoâ€¦)

### ğŸ”¹ Lo que hace EM

-   **E-step**:\
    \[\
    \\mathbb{E}\[z\_{h,b} \\mid y_b, \\theta\]\
    \]

-   **M-step**:\
    \[\
    \\arg\\max\_\\beta \\sum\_{h,b} \\mathbb{E}\[z\_{h,b}\] \\log p(z\_{h,b} \\mid X_h)\
    \]

ğŸ“Œ Todo lo demÃ¡s del curso es *implementar esto bien*.

-------------------------------------

perfecto asi. tengo 2 preguntas: 1) En Mixture Models explicas ğ‘§
â„
,
ğ‘
z
h,b
	â€‹

= responsabilidades

pero no aprece en la formula (aparece p_h) 2) En pocas lineas, dime sobre credit attribution y relacion con MTA en marketing - solo para entender, no entraremos mas en este curso

-----------------------------------------------------

Muy buenas preguntas â€” ambas van **al nÃºcleo conceptual** del modelo. Las respondo con precisiÃ³n y sin desviar el curso.

## 1ï¸âƒ£ Mixture Models: Â¿dÃ³nde estÃ¡n exactamente las *responsabilidades* ( z\_{h,b} )?

Tienes razÃ³n: **tal como estaba escrita**, la fÃ³rmula era **incompleta / implÃ­cita**. Vamos a corregirla y dejarla *canÃ³nica*.

### ğŸ”¹ Mixture model estÃ¡ndar (recordatorio)

En un mixture clÃ¡sico:

\[\
p(y) = \\sum\_{k=1}\^K \\pi_k , p(y \\mid \\theta_k)\
\]

y la **variable latente** es:\
\[\
z\_{ik} \\in {0,1}, \\quad \\sum_k z\_{ik} = 1\
\]

Las **responsabilidades** son:\
\[\
\\gamma\_{ik} = \\mathbb{E}\[z\_{ik} \\mid y_i\]\
\]

### ğŸ”¹ Mixture reinterpretado en tu problema HCPâ€“Brick

AquÃ­:

-   Un **brick** genera ventas

-   Cada **venta/unidad** proviene de *un* HCP

-   El HCP es el â€œcomponenteâ€ del mixture

#### Variable latente correcta

\[\
z\_{h,b} \\equiv \\text{nÃºmero de unidades del brick } b \\text{ atribuibles al HCP } h\
\]

No es binaria, es **conteo latente**.

### ğŸ”¹ Modelo correctamente escrito

Para un brick ( b ):

\[\
y_b = \\sum\_{h \\in b} z\_{h,b}\
\]

y la probabilidad marginal:

\[\
p(y_b \\mid \\theta) =\
\\sum\_{{z\_{h,b}}:\\sum z = y_b}\
\\prod\_{h \\in b}\
\\left\[\
\\pi\_{h,b}\^{z\_{h,b}} \\cdot p(z\_{h,b} \\mid \\theta_h)\
\\right\]\
\]

ğŸ‘‰ Esto es un **mixture con datos agregados**.

### ğŸ”¹ Responsabilidades (E-step real)

Las **responsabilidades** NO aparecen directamente en la likelihood, sino en el **E-step**:

\[\
\\gamma\_{h,b}\
;\\equiv;\
\\mathbb{E}\[z\_{h,b} \\mid y_b, \\theta\]\
\]

IntuiciÃ³n:

-   ( p_h ) describe **cÃ³mo genera ventas un HCP**

-   ( \\gamma\_{h,b} ) dice **quÃ© fracciÃ³n del brick le pertenece**

En Poisson, por ejemplo:

# \[ \\gamma\_{h,b}

y_b \\cdot\
\\frac{\\lambda_h w\_{h,b}}{\\sum\_{h' \\in b} \\lambda\_{h'} w\_{h',b}}\
\]

ğŸ“Œ **ConclusiÃ³n**

-   ( p_h ) = distribuciÃ³n del componente

-   ( z\_{h,b} ) = variable latente

-   ( \\gamma\_{h,b} = \\mathbb{E}\[z\_{h,b}\] ) = responsabilidades

Gracias por seÃ±alarlo: es una distinciÃ³n crÃ­tica.

## 2ï¸âƒ£ Credit attribution y relaciÃ³n con MTA (en pocas lÃ­neas)

### ğŸ”¹ Credit attribution (idea general)

Problema:

> Un resultado final (venta, revenue) es consecuencia de **mÃºltiples agentes**, pero solo observamos el total.

Objetivo:

-   Repartir el crÃ©dito entre agentes de forma consistente y explicable.

### ğŸ”¹ Multi-Touch Attribution (Marketing)

En marketing digital:

-   Agentes = canales (Search, Display, Emailâ€¦)

-   Observado = conversiÃ³n final

-   Latente = contribuciÃ³n de cada touch

Modelos MTA modernos:

-   Usan **mixture models**

-   Usan **EM o Bayesian inference**

-   Estiman *responsabilidades* por canal

### ğŸ”¹ AnalogÃ­a exacta con tu problema

| Marketing MTA | HCPâ€“Brick          |
|---------------|--------------------|
| Canales       | HCPs               |
| Touches       | Presencia en brick |
| ConversiÃ³n    | Ventas del brick   |
| Attribution   | ( z\_{h,b} )       |

ğŸ“Œ Conceptualmente **idÃ©ntico**, solo cambia la semÃ¡ntica del dominio.

## âœ… Checkpoint final

Si estÃ¡s de acuerdo:\
â¡ï¸ pasamos al **CapÃ­tulo 2 â€” Modelo probabilÃ­stico base**\
(donde fijamos **Poisson vs Gamma**, likelihood formal y supuestos).

ConfÃ­rmame y arrancamos.