# El problema como modelo de variables latentes

## Introducción

Este capítulo traduce el problema operativo “ventas agregadas por brick, pero decisión a nivel HCP” a un lenguaje estadístico preciso. La idea central es que existe una descomposición que no observamos: las contribuciones individuales $z_{h,b}$ que, al agregarse, recuperan exactamente el total observado $y_b$. A partir de esa formulación, veremos por qué enfoques directos (regresión sobre el agregado o attribution post-hoc) no capturan bien la estructura del problema, y por qué EM aparece como el algoritmo natural para alternar entre reparto esperado (*E-step*) y actualización de productividad (*M-step*).

## Reformulación del business problem (de negocio → estadística)

### Formulación de negocio

El punto de partida es una situación recurrente: tenemos **HCPs** distribuidos en **bricks**, y cada brick reporta **ventas totales agregadas**. Sin embargo, no observamos ventas a nivel HCP. Aun así, el objetivo operativo suele ser asignar ese total a HCPs individuales de forma defendible. Para hacerlo, normalmente contamos con **features por HCP** (por ejemplo, especialidad, visitas o demografía).

En este libro, **HCP** significa *Health Care Professional* (profesional sanitario) y **brick** es la unidad de agregación territorial/comercial en la que se reporta el total (una “zona” o “segmento” operativo).

### Reformulación estadística

Esto no es un problema de predicción directa, sino de **inferir una descomposición no observada**.

> “Las ventas de cada brick son la suma de contribuciones latentes de los HCPs que lo componen.”

Formalmente:

Existe un **proceso generativo no observado**: cada HCP genera ventas, y parte de esas ventas se manifiestan en cada brick. El problema es que solo observamos el **resultado agregado**.

Esto es exactamente un problema de **variables latentes** con **datos incompletos**.

## Qué está observado y qué es latente

Llamamos $y_b$ a las ventas totales del brick $b$. Para cada HCP $h$ disponemos de covariables $X_h$. Además, representamos la pertenencia HCP–brick con $M_{h,b} \in \{0,1\}$ (o un peso si la pertenencia no es binaria).

La variable que falta en los datos es precisamente la que queremos inferir: $z_{h,b}$, la contribución de ventas del HCP $h$ al brick $b$. Esta variable latente está sujeta a una restricción estructural:

$$
\sum_{h \in b} z_{h,b} = y_b
$$

Este $z_{h,b}$ es **la variable latente clave**.

## Por qué esto NO es una regresión directa

### Regresión naïve

Podrías intentar algo como:

$$
y_b = \sum_{h \in b} f(X_h)
$$

Esta idea falla por motivos estructurales. En primer lugar, no identifica contribuciones individuales: varias descomposiciones pueden encajar el mismo agregado. En segundo lugar, no hay una likelihood bien definida que capture el mecanismo “agregado como suma de contribuciones”. En tercer lugar, no aparece de manera natural un reparto que conserve masa (esto es, que “sume exactamente” a $y_b$ por construcción). En conjunto, es un planteamiento pobre para inferencia.

### SHAP / Attribution post-hoc

Otra tentación es usar atribución post-hoc (por ejemplo, SHAP). El problema es que SHAP está formulado para escenarios de **predicción individual** con una función objetivo conocida y observaciones independientes. Aquí ocurre lo contrario: el target es **agregado**, la unidad de decisión (HCP) no tiene label, y el problema es **estructuralmente inverso**.

SHAP responde *“por qué el modelo predijo”*,\
no *“cómo se generaron los datos”*.

## Por qué EM es el algoritmo natural

### Observación clave

Si conociéramos $z_{h,b}$, podríamos estimar con facilidad la productividad de los HCPs y ajustar un GLM estándar. Si, por el contrario, conociéramos la productividad de los HCPs, podríamos repartir las ventas esperadas entre ellos dentro de cada brick. EM alterna exactamente entre estas dos piezas.

### EM en una frase (en este contexto)

> EM alterna entre **atribuir probabilísticamente ventas a HCPs** (*E-step*)\
> y **actualizar el modelo de productividad de HCPs** (*M-step*)

## Relación con modelos conocidos

### Mixture models

Cada venta (o unidad) del brick puede interpretarse como generada por un **componente** (un HCP). En un mixture model estándar:

$$
p(y) = \sum_{k=1}^K \pi_k\, p(y\mid \theta_k).
$$

La variable latente asociada a la asignación a componentes es, típicamente:

$$
z_{ik} \in \{0,1\}, \quad \sum_k z_{ik}=1,
$$

y las responsabilidades son:

$$
\gamma_{ik} = \mathbb{E}[z_{ik}\mid y_i].
$$

En el problema HCP–brick, la variable latente relevante no es binaria, sino un **conteo latente**:

$$
z_{h,b} \equiv \text{número de unidades del brick } b \text{ atribuibles al HCP } h,
$$

con la restricción estructural:

$$
y_b = \sum_{h\in b} z_{h,b}.
$$

Una forma canónica de expresar la marginal (datos agregados) es:

$$
p(y_b\mid \theta)=
\sum_{\{z_{h,b}\}:\,\sum z=y_b}
\prod_{h\in b}
\left[\pi_{h,b}^{z_{h,b}}\, p(z_{h,b}\mid \theta_h)\right].
$$

En este contexto, las responsabilidades aparecen en el *E-step* como:

$$
\gamma_{h,b} \equiv \mathbb{E}[z_{h,b}\mid y_b,\theta].
$$

Por ejemplo, en el caso Poisson (capítulos posteriores):

$$
\gamma_{h,b}=y_b\cdot
\frac{\lambda_h w_{h,b}}{\sum_{h'\in b} \lambda_{h'} w_{h',b}}.
$$

En resumen: $p(\cdot\mid \theta_h)$ describe el componente, $z_{h,b}$ es la variable latente, y $\gamma_{h,b}=\mathbb{E}[z_{h,b}]$ son las responsabilidades.

### Ecological inference

Este es también un caso clásico de *ecological inference*: se dispone de datos agregados, se desea inferir cantidades a nivel individual, y existen múltiples soluciones compatibles con el agregado. La diferencia es que aquí se impone un **modelo generativo**, y la regularización vía covariables ayuda a evitar soluciones arbitrarias.

### Credit attribution

El encuadre también es cercano a *credit attribution*: repartir un resultado final (venta o revenue) entre varios agentes. La versión marketing de este problema es el *multi-touch attribution* (MTA), donde se reparte contribución entre canales/campañas. La diferencia clave aquí es que las “touches” no se observan: solo vemos el resultado final agregado.

## Definición clara del modelo generativo

Este es el **modelo conceptual que usaremos**:

### Nivel HCP

Cada HCP tiene una **productividad latente**:

$$
\theta_h = f(X_h; \beta)
$$

En el caso de unidades, típicamente $\theta_h=\lambda_h$ (Poisson). En el caso de euros, $\theta_h$ representa una parametrización análoga (por ejemplo, la media) bajo una Gamma o una Lognormal.

### Nivel brick

Las ventas del brick se generan como:

$$
y_b = \sum_{h \in b} z_{h,b}
$$

con:

$$
z_{h,b} \sim \text{Distribución}(\theta_h, w_{h,b})
$$

donde:

$w_{h,b}$ es la exposición (o peso) del HCP en el brick, que puede recoger visitas, tiempo u otras medidas de oportunidad (incluyendo overlap geográfico).

### Lo que hace EM

Con esta formulación, EM se expresa como dos pasos alternos. En el *E-step*, con $\theta$ fijo, se calcula la atribución esperada dentro de cada brick:

$$
\mathbb{E}[z_{h,b} \mid y_b, \theta].
$$

En el *M-step*, con $\mathbb{E}[z]$ fijo, se actualizan los parámetros (por ejemplo $\beta$) resolviendo:

$$
\arg\max_\beta \sum_{h,b} \mathbb{E}[z_{h,b}] \log p(z_{h,b} \mid X_h).
$$

Todo lo demás del curso es implementar esto bien.

## Nota breve: relación con MTA (solo para situar el concepto)

El *credit attribution* es el problema de repartir un resultado final (venta, revenue) entre múltiples agentes cuando solo se observa el total. En marketing, la versión típica es *Multi-touch Attribution (MTA)*: repartir conversiones o contribución entre canales/campañas/creatividades a partir de señales de exposición.

La analogía con el caso HCP–brick es directa:

| Marketing MTA | HCP–Brick          |
|---------------|--------------------|
| Canales       | HCPs               |
| Touches       | Presencia en brick |
| Conversión    | Ventas del brick   |
| Attribution   | $z_{h,b}$          |

Conceptualmente es el mismo patrón “agregado → desagregado”; cambia la semántica del dominio.

## Resumen

- El dato observado es **agregado** ($y_b$), pero el objetivo es inferir una **descomposición no observada** a nivel HCP ($z_{h,b}$).
- La variable latente $z_{h,b}$ está sujeta a una restricción estructural: **conservación de masa** ($\sum_{h\in b} z_{h,b}=y_b$).
- No es una regresión directa: sin un mecanismo “agregado como suma de contribuciones” no aparece una likelihood ni un reparto que sume exactamente a $y_b$.
- EM alterna entre calcular atribución esperada dentro de cada brick (*E-step*) y actualizar productividad/coeficientes (*M-step*).
- Este patrón “agregado → desagregado” es el mismo que en *credit attribution* (y conecta de forma directa con MTA).