# Mock data generation

## Introducción

La idea de este capítulo es generar un mundo sintético donde conocemos la **verdad** del proceso generativo. Esto permite entrenar EM en condiciones controladas y cuantificar qué recupera (y qué no) cuando solo se observa el agregado por brick.

En concreto, en el mundo simulado fijamos:

-   Features por HCP ($X_h$)
-   Parámetros “reales” ($\beta^*$)
-   Contribuciones latentes reales ($z^*_{h,b}$)
-   Ventas observadas por brick ($y_b$)

A partir de estas cantidades, podemos medir si EM recupera:

1.  $\beta^*$ (productividad estructural)

2.  $z^*_{h,b}$ (atribución)

3.  $y_b$ (consistencia y calibración)

## Diseño de simulación (qué queremos que sea realista)

### HCPs

-   **Especialidad** (categoría): GP / Cardio / Endo / …
-   **Seniority** (numérica u ordinal)
-   **Visitas** (conteo; y pueden variar por brick)
-   Opcional: “potencial” (continua), “segmento” (A/B/C)

### Bricks

-   Composición heterogénea:

    -   bricks “densos” con muchos HCPs
    -   bricks “pequeños” con pocos HCPs
    -   overlaps: un HCP puede estar en varios bricks

-   Exposición ($w_{h,b}$):

    -   0 si no pertenece
    -   si pertenece: proporcional a visitas u otro proxy

### Verdadera productividad

En unidades, modelamos conteos con Poisson. Primero definimos la productividad por HCP como:

$$
\lambda_h^{*} = \exp(X_h\beta^{*}).
$$

A continuación generamos contribuciones latentes por par $(h,b)$ y observamos el agregado por brick:

$$
z_{h,b}^{*} \sim \operatorname{Poisson}(\lambda_h^{*} w_{h,b})
\quad;\quad
y_b = \sum_{h\in \mathcal{H}_b} z_{h,b}^{*}.
$$

## Código Python: generador reutilizable (unidades / Poisson)

El siguiente código genera un dataset listo para entrenar EM, comparar contra *ground truth* y hacer stress tests.

```python
import numpy as np
import pandas as pd


def simulate_hcp_brick_poisson(
    n_hcps=400,
    n_bricks=80,
    avg_bricks_per_hcp=2.5,
    avg_hcps_per_brick=18,
    seed=7,
):
    rng = np.random.default_rng(seed)

    # ----- (A) HCP features -----
    specialties = np.array(["GP", "CARDIO", "ENDO", "ONCO", "PULMO"])
    spec = rng.choice(specialties, size=n_hcps, p=[0.45, 0.18, 0.15, 0.12, 0.10])

    seniority = rng.integers(0, 21, size=n_hcps)  # years (0..20)
    potential = rng.lognormal(mean=0.0, sigma=0.6, size=n_hcps)  # positive continuous

    # One-hot for specialty
    spec_df = pd.get_dummies(spec, prefix="spec")
    X = np.column_stack(
        [
            np.ones(n_hcps),  # intercept
            seniority / 10.0,  # scaled
            np.log1p(potential),  # log feature
            spec_df.to_numpy(),  # dummies
        ]
    )
    feature_names = (
        ["intercept", "seniority_x0.1", "log1p_potential"] + list(spec_df.columns)
    )

    p = X.shape[1]

    # ----- (B) True beta* (ground truth) -----
    beta_true = np.zeros(p)
    beta_true[0] = -0.3  # intercept
    beta_true[1] = 0.20  # seniority helps
    beta_true[2] = 0.55  # potential helps

    # Specialty effects (relative to baseline; here baseline is implicitly captured by intercept)
    # Put mild lifts / penalties
    for j, name in enumerate(feature_names):
        if name == "spec_CARDIO":
            beta_true[j] = 0.25
        elif name == "spec_ONCO":
            beta_true[j] = 0.35
        elif name == "spec_PULMO":
            beta_true[j] = 0.10
        elif name == "spec_ENDO":
            beta_true[j] = 0.15
        # GP left at 0 shift (reference-ish), though with full one-hot it's not a strict reference.
    # (With full one-hot + intercept, there is collinearity; in practice you drop one dummy.
    # For simulation it's fine; for fitting, use a reference category or regularize.)

    # True productivity per HCP
    lambda_true = np.exp(X @ beta_true)

    # ----- (C) Brick membership + exposure w_{h,b} -----
    # Goal: heterogeneous bricks + overlaps.
    # We'll assign each HCP to K bricks, K ~ Poisson(avg_bricks_per_hcp) truncated >=1
    K = rng.poisson(lam=avg_bricks_per_hcp, size=n_hcps)
    K = np.clip(K, 1, max(1, n_bricks // 2))

    # Popularity of bricks (some bricks are "bigger")
    brick_pop = rng.lognormal(mean=0.0, sigma=0.7, size=n_bricks)
    brick_pop = brick_pop / brick_pop.sum()

    # Build sparse membership list
    rows = []
    for h in range(n_hcps):
        bricks_h = rng.choice(n_bricks, size=K[h], replace=False, p=brick_pop)
        # visits per (h,b): more variable, depends on seniority & random
        # visits = base + seniority effect + noise, then clipped >=0
        base = rng.poisson(lam=2.5)  # baseline
        for b in bricks_h:
            visits = base + rng.poisson(lam=0.15 * seniority[h] + 1.0)
            # exposure: use visits directly (or normalized variant)
            w_hb = float(visits)
            if w_hb == 0:
                w_hb = 0.5  # keep small exposure if assigned (optional)
            rows.append((h, b, visits, w_hb))

    hb = pd.DataFrame(rows, columns=["hcp_id", "brick_id", "visits", "w"])

    # Make bricks heterogeneous: optionally drop/keep to target avg_hcps_per_brick
    # We'll just report achieved distribution; simulation already creates variation.

    # ----- (D) Generate latent z* and observed y -----
    # z_{h,b}^{*} ~ Poisson(lambda_true[h] * w[h,b])
    mu = lambda_true[hb["hcp_id"].to_numpy()] * hb["w"].to_numpy()
    z_true = rng.poisson(lam=mu)

    hb["z_true"] = z_true

    # Aggregate observed sales by brick
    y_by_brick = hb.groupby("brick_id")["z_true"].sum().rename("y").reset_index()

    # Pack HCP table
    hcp = pd.DataFrame(
        {
            "hcp_id": np.arange(n_hcps),
            "specialty": spec,
            "seniority": seniority,
            "potential": potential,
            "lambda_true": lambda_true,
        }
    )
    for j, nm in enumerate(feature_names):
        hcp[nm] = X[:, j]

    meta = {
        "feature_names": feature_names,
        "beta_true": beta_true,
        "n_hcps": n_hcps,
        "n_bricks": n_bricks,
    }

    return hcp, hb, y_by_brick, meta


# Example usage
hcp_df, hb_df, y_df, meta = simulate_hcp_brick_poisson(seed=42)
print(hcp_df.head())
print(hb_df.head())
print(y_df.head())
print("beta_true:", meta["beta_true"])
```

### Qué te devuelve

-   `hcp_df`: tabla HCP con features y `lambda_true`

-   `hb_df`: tabla de relaciones HCP–brick con `w`, `visits`, y **ground truth** `z_true`

-   `y_df`: ventas observadas por brick (lo único que normalmente tendrías)

-   `meta`: `beta_true` y nombres de features

## Variantes útiles (para stress testing)

Puedes hacer el mundo más “real” con:

-   **Overdispersion** (si Poisson se queda corto):

    -   usar Negative Binomial para $z_{h,b}$

-   **Zero inflation**:

    -   con probabilidad $p$, $z_{h,b}=0$

-   **Brick effects**:

    -   añadir un factor por brick ($a_b$): ($\mu_{h,b} = \lambda_h^{*} w_{h,b} a_b$)

-   **Dependencia en visitas**:

    -   que visitas dependan de especialidad o potencial

Estas variantes pueden añadirse posteriormente para testear robustez.

## Cómo evaluar si EM “recupera la verdad”

Al entrenar EM con **solo** ($y_b$), ($w_{h,b}$) y ($X_h$), se puede comparar:

### A) Recuperación de parámetros (si usas features)

-   Error en $\beta$:

    -   RMSE: $\lVert\hat\beta - \beta^*\rVert$

    -   o correlación entre $X\hat\beta$ y $X\beta^*$

**Recomendación práctica:** comparar la productividad en escala log:

-   comparar $\eta_h = X_h\beta$ (log-rate), suele ser más estable.

### B) Recuperación de atribución

Comparas matrices (en formato long) entre:

-   $z^*_{h,b}$ vs $\hat z_{h,b} = \mathbb{E}[z_{h,b}\mid y,\hat{\beta}]$

Métricas típicas:

-   Correlación por (h,b)

-   RMSE por (h,b)

-   Error relativo por brick (aunque por construcción debería sumar bien)

### C) Chequeos de calibración (must-have)

-   Por brick: $\sum_h \hat z_{h,b} = y_b$ (debe cumplirse numéricamente)

-   Distribución de residuales (si introduces overdispersion)

### D) Métricas de likelihood

-   Log-likelihood observada:

    $$
    \sum_b \log \operatorname{Poisson}\Big(y_b \mid \sum_h \hat\lambda_h w_{h,b}\Big)
    $$

-   Monitorizar que EM la **no disminuye** por iteración (sanity check)

## Nota: versión “euros” (Gamma) en mock data

Para simular euros:

-   genera $z_{h,b}$ Gamma con media proporcional a $\theta_h w_{h,b}$

-   suma por brick para obtener $y_b$

La evaluación es igual, pero:

-   compara en escala log o con métricas robustas (outliers pesan más).

## Estructura de los datasets simulados

El generador devuelve tres tablas. Conviene entenderlas porque corresponden a los tres “niveles” que aparecen en el modelo: HCP (productividad), HCP–brick (exposición y latentes) y brick (agregado observado).

### `hcp_df`: tabla a nivel HCP

`hcp_df` contiene **una fila por HCP**. Mezcla variables observables (que existirían en producción) con variables de *ground truth* (solo disponibles en simulación).

- `hcp_id`: identificador del HCP. Sirve para joins con `hb_df` y no entra al modelo.
- `specialty`: especialidad médica (categórica). En el modelo se representa con dummies `spec_*`.
- `seniority`: años de experiencia (numérica). Se escala como `seniority_x0.1 = seniority / 10` para estabilidad.
- `potential`: proxy continuo de potencial comercial. Se transforma como `log1p_potential = \log(1+\texttt{potential})`.

Además, `hcp_df` incluye columnas de diseño (la matriz $X_h$): `intercept`, `seniority_x0.1`, `log1p_potential` y `spec_*`. En producción, con intercept y dummies completas hay colinealidad y conviene fijar una categoría de referencia o usar regularización.

Por último, `lambda_true` es **ground truth** (no observable en producción):

$$
\lambda_h^{*} = \exp(X_h\beta^{*}).
$$

Una lectura mental útil es: `hcp_df` describe **qué esperamos de cada HCP en promedio**; el algoritmo no observa ventas por HCP y, por tanto, no puede “aprender ventas” individuales directamente.

### `hb_df`: tabla a nivel HCP–brick

`hb_df` está en formato long y representa **relaciones HCP–brick** (una fila por par $(h,b)$ con pertenencia). Aquí viven la exposición y, en simulación, los latentes reales.

- `hcp_id`: FK hacia `hcp_df.hcp_id`.
- `brick_id`: FK hacia `y_df.brick_id`. Define los conjuntos $\mathcal{H}_b$.
- `visits`: proxy observado de oportunidad/esfuerzo.
- `w`: exposición efectiva $w_{h,b}$ (en la simulación, aproximadamente `w ≈ visits`, con un pequeño suelo para evitar ceros).
- `z_true`: latente real (solo en simulación), generado como:

$$
z_{h,b}^{*} \sim \operatorname{Poisson}(\lambda_h^{*} w_{h,b}).
$$

Esta tabla fija “quién compite con quién” dentro de cada brick y es donde vive el *E-step*.

| Columna    | Nivel     | Disponible en datos reales | Rol                     |
|------------|-----------|----------------------------|-------------------------|
| `hcp_id`   | ID        | Sí                       | Join                    |
| `brick_id` | ID        | Sí                       | Join                    |
| `visits`   | Observada | Sí                       | Proxy esfuerzo          |
| `w`        | Observada | Sí                       | Exposición en el modelo |
| `z_true`   | Latente   | No                       | Ground truth            |

### `y_df`: tabla a nivel brick (agregado observado)

`y_df` representa lo que típicamente está disponible en datos reales: el total por brick.

- `brick_id`: identificador del brick.
- `y`: ventas agregadas del brick, definidas por:

$$
y_b = \sum_{h\in \mathcal{H}_b} z_{h,b}^{*}.
$$

| Columna    | Disponible en datos reales | Rol              |
|------------|----------------------------|------------------|
| `brick_id` | Sí                       | Identificador    |
| `y`        | Sí                       | Ventas agregadas |

### Cómo encajan las tres tablas (lectura operativa)

```text
hcp_df  -> define λ_h^* = exp(X_h β^*)
hb_df   -> define exposición por par (h,b): w_{h,b}
y_df    -> observa solo el agregado por brick: y_b
```

En este esquema, el *E-step* opera sobre `hb_df` (reparto esperado dentro de cada brick) y el *M-step* opera sobre `hcp_df` (actualización de $\beta$ si hay covariables). La likelihood observada corresponde a `y_df`.

## Resumen

- El objetivo del mock data es disponer de un mundo sintético con *ground truth* para evaluar recuperación de parámetros, atribución latente y calibración del agregado.
- El generador produce tres tablas: `hcp_df` (features y productividad verdadera), `hb_df` (estructura HCP–brick, exposición y latentes simuladas) y `y_df` (ventas agregadas por brick).
- En unidades, el mundo simulado usa $\lambda_h^{*}=\exp(X_h\beta^{*})$ y genera $z_{h,b}^{*}\sim\operatorname{Poisson}(\lambda_h^{*} w_{h,b})$ con $y_b=\sum_{h\in\mathcal{H}_b} z_{h,b}^{*}$.
- Para stress testing, se pueden introducir overdispersion, zero inflation y efectos de brick sin cambiar la lógica general del pipeline.
- La evaluación debe incluir recuperación de $\beta$ (cuando hay covariables), recuperación de $z$ en formato long y chequeos de conservación de masa $\sum_h \hat z_{h,b}=y_b$.