# Modelo realista: covariables, regularización y estabilidad

## Introducción

Este capítulo es **el salto a producción**. Pasamos de “un $\lambda_h$ libre por HCP” a un modelo **paramétrico, generalizable y regularizado**.

## Por qué necesitamos covariables (motivación)

En el Capítulo 4 se ve un problema clave: los HCPs “exclusivos” de bricks grandes pueden absorber demasiada venta, el modelo es **débilmente identificable** y no generaliza a HCPs nuevos. La salida natural es **atar la productividad a información observable**.

## Nueva definición de productividad

Definimos:

$$
\boxed{\lambda_h = \exp(X_h \beta)}
$$

En esta parametrización:

-   $X_h \in \mathbb{R}^p$: features del HCP\
    (especialidad, seniority, nº visitas, potencial, etc.)
-   $\beta \in \mathbb{R}^p$: coeficientes globales

La interpretación es directa: dos HCPs con los mismos $X$ tienen la misma productividad esperada, y el modelo **comparte fuerza estadística**.

## Modelo completo con covariables (Poisson)

### Modelo generativo

$$
z_{h,b} \sim \operatorname{Poisson}(\exp(X_h\beta),\, w_{h,b})
$$

$$
y_b = \sum_{h\in b} z_{h,b}
$$

## EM + GLM: cómo encajan

### E-step (igual que antes)

Con $\lambda_h^{(t)} = \exp(X_h\beta^{(t)})$:

$$
\boxed{
\mathbb{E}\big[z_{h,b}\mid y_b,\beta^{(t)}\big]
= y_b\cdot
\frac{\exp(X_h\beta^{(t)})\, w_{h,b}}
{\sum_{h'\in b}\exp(X_{h'}\beta^{(t)})\, w_{h',b}}
}
$$

En este punto, **nada cambia conceptualmente**.

### M-step (ahora es un GLM)

Definimos los “pseudo-targets”:

$$
\widetilde{y}_h = \sum_b \mathbb{E}[z_{h,b}]
$$

y la exposición total:

$$
\widetilde{w}_h = \sum_b w_{h,b}
$$

Entonces el M-step es:

$$
\boxed{
\max_{\beta} \sum_h \left[ \widetilde{y}_h \log \lambda_h - \lambda_h \widetilde{w}_h \right]
\quad
\mathrm{con}\ \lambda_h = \exp(X_h\beta)
}
$$

Esto es exactamente una **Poisson regression con offset**:

-   Response: $\widetilde{y}_h$
-   Offset: $\log(\widetilde{w}_h)$

Se resuelve con IRLS estándar.

## Caso euros (Gamma + covariables)

Análogo:

$$
z_{h,b} \sim \operatorname{Gamma}(k,\, \theta_h w_{h,b})
\quad
\mathrm{con}\ \theta_h = \exp(X_h\beta)
$$

-   E-step: reparto proporcional a $\theta_h w_{h,b}$
-   M-step:
    -   $\beta$: GLM Gamma con link log
    -   $k$: fijo o estimado con Newton

Misma lógica, distinta distribución.

## Regularización (crítica en la práctica)

Sin regularización, $\beta$ puede explotar, especialmente si hay features raras o colineales.

### Penalización típica

$$
\beta \sim \mathcal{N}(0,\sigma^2 I)
$$

Esto es equivalente a ridge (L2) y a MAP en versión bayesiana. En producción, es muy recomendable.

## Identificabilidad (qué se arregla y qué no)

### Lo que mejora

-   HCPs exclusivos ya no “absorben” arbitrariamente
-   Productividades están ancladas a $X_h$
-   Generaliza a HCPs nuevos

### Lo que sigue siendo cierto

-   Attribution ≠ causalidad
-   Si dos HCPs tienen $X$ casi idéntico y siempre co-ocurren, no se distinguen

## Qué pasa cuando un HCP está en muchos bricks

Esto es **bueno**, no malo. Un HCP que aparece en muchos bricks entra en muchas ecuaciones,

$$
y_b \approx \sum_h \exp(X_h\beta)\, w_{h,b}
$$

por lo que el modelo puede usarlo como ancla y reducir la varianza de $\beta$. En la práctica, los HCPs “centrales” estabilizan el ajuste, y los HCPs raros se benefician del *pooling*.

## Salida — Modelo listo para datos reales

### Modelo final (unidades)

$$
\boxed{
\begin{aligned}
z_{h,b} &\sim \operatorname{Poisson}(\exp(X_h\beta),\, w_{h,b}) \\
y_b &= \sum_{h\in b} z_{h,b}
\end{aligned}
}
$$

con EM para inferencia, GLM en el M-step y regularización opcional.

## Lectura de negocio (muy importante)

> “Estamos estimando un **modelo probabilístico de generación de ventas**,\
> usando información observable del HCP,\
> y la attribution es la **mejor explicación consistente** con ese modelo.”

## Resumen

- Este capítulo reemplaza el parámetro libre $\lambda_h$ por una productividad estructurada $\lambda_h=\exp(X_h\beta)$ para ganar generalización y estabilidad.
- El *E-step* se mantiene; el *M-step* se convierte en un ajuste de GLM (Poisson con offset) a partir de pseudo-targets $\widetilde{y}_h$ y exposición agregada $\widetilde{w}_h$.
- La regularización sobre $\beta$ es crítica en la práctica y mejora la identificabilidad.
- Esto no convierte la attribution en causalidad ni resuelve casos en los que dos HCPs son prácticamente indistinguibles por sus $X$ y su patrón de co-ocurrencia.