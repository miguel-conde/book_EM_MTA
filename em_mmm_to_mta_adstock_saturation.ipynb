{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EM para desagregar contribuciones incrementales de MMM a MTA (campañas/creatividades)\n",
        "\n",
        "Este notebook muestra un enfoque **top-down**:\n",
        "\n",
        "- Un MMM produce contribuciones incrementales por **canal/medio** a nivel **tiempo** (y opcionalmente **geo**):  \n",
        "  \\( C_{m,t,g}^{inc} \\)\n",
        "\n",
        "- Queremos repartir \\( C_{m,t,g}^{inc} \\) a niveles inferiores (campañas/creas) \\(k\\) dentro del medio \\(m\\):  \n",
        "  \\( z_{m,k,t,g} \\), con la restricción:  \n",
        "  \\( \\sum_k z_{m,k,t,g} = C_{m,t,g}^{inc} \\)\n",
        "\n",
        "- Usamos como “exposición” una señal de delivery (impresiones/GRPs) **transformada como en MMM**:\n",
        "  - **Adstock** (carryover)\n",
        "  - **Saturación** (rendimientos decrecientes, p. ej. Hill)\n",
        "\n",
        "El reparto (E-step) se hace proporcional a:\n",
        "\\[\n",
        "\\lambda_{m,k}\\;\\cdot\\; s(\\text{Adstock}(x_{m,k,t,g}))\n",
        "\\]\n",
        "donde \\(x\\) son impresiones/GRPs y \\(s(\\cdot)\\) es saturación.\n",
        "\n",
        "> Nota causal: si el MMM es incremental/causal a nivel canal, el total \\(C^{inc}\\) lo es.  \n",
        "> El split intra-canal es **atribución model-based**, no causal garantizado, salvo que haya identificación adicional intra-canal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Utilidades: Adstock y Saturación (Hill)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def adstock_geometric(x, theta):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    a = np.zeros_like(x)\n",
        "    for t in range(len(x)):\n",
        "        a[t] = x[t] + (theta * a[t-1] if t > 0 else 0.0)\n",
        "    return a\n",
        "\n",
        "def hill_saturation(x, alpha, gamma):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x_pos = np.clip(x, 0.0, None)\n",
        "    xa = np.power(x_pos + 1e-12, alpha)\n",
        "    ga = np.power(gamma + 1e-12, alpha)\n",
        "    return xa / (xa + ga)\n",
        "\n",
        "def effective_exposure(x, theta, alpha, gamma):\n",
        "    a = adstock_geometric(x, theta=theta)\n",
        "    s = hill_saturation(a, alpha=alpha, gamma=gamma)\n",
        "    return a, s\n",
        "\n",
        "x = np.array([0, 10, 0, 0, 5, 0], dtype=float)\n",
        "a, s = effective_exposure(x, theta=0.6, alpha=1.2, gamma=8.0)\n",
        "pd.DataFrame({\"x\": x, \"adstock\": a, \"sat\": s})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Datos mock: canal (medio) -> campañas, con tiempo x geo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "M = 2\n",
        "K_per_m = 6\n",
        "T = 52\n",
        "G = 5\n",
        "\n",
        "media_names = [\"Search\", \"Social\"]\n",
        "geos = [f\"G{i+1}\" for i in range(G)]\n",
        "weeks = np.arange(T)\n",
        "\n",
        "theta_m = {\"Search\": 0.40, \"Social\": 0.65}\n",
        "alpha_m = {\"Search\": 1.20, \"Social\": 1.40}\n",
        "gamma_m = {\"Search\": 0.35, \"Social\": 0.25}\n",
        "\n",
        "campaign_rows = []\n",
        "for m in media_names:\n",
        "    for k in range(K_per_m):\n",
        "        campaign_rows.append({\n",
        "            \"medium\": m,\n",
        "            \"campaign\": f\"{m}_C{k+1}\",\n",
        "            \"creative_family\": rng.choice([\"A\", \"B\", \"C\"]),\n",
        "            \"objective\": rng.choice([\"Awareness\", \"Consideration\", \"Conversion\"], p=[0.3,0.4,0.3]),\n",
        "        })\n",
        "campaign_df = pd.DataFrame(campaign_rows)\n",
        "\n",
        "geo_effect = rng.lognormal(mean=0.0, sigma=0.25, size=G)\n",
        "time_season = 1.0 + 0.25*np.sin(2*np.pi*weeks/T) + 0.10*np.sin(4*np.pi*weeks/T)\n",
        "\n",
        "x_rows = []\n",
        "for _, row in campaign_df.iterrows():\n",
        "    m = row[\"medium\"]\n",
        "    k = row[\"campaign\"]\n",
        "    base = rng.lognormal(mean=0.0, sigma=0.5)\n",
        "    for g_idx, g in enumerate(geos):\n",
        "        level = base * geo_effect[g_idx] * rng.lognormal(mean=0.0, sigma=0.15)\n",
        "        x_t = level * time_season * rng.lognormal(mean=0.0, sigma=0.25, size=T)\n",
        "        mask = rng.random(T) < 0.08\n",
        "        x_t[mask] = 0.0\n",
        "        for t in range(T):\n",
        "            x_rows.append({\"medium\": m, \"campaign\": k, \"geo\": g, \"t\": int(t), \"x_raw\": float(x_t[t])})\n",
        "\n",
        "x_df = pd.DataFrame(x_rows)\n",
        "\n",
        "# Scale per medium to stabilize Hill gamma interpretation\n",
        "x_df[\"x_scaled\"] = x_df[\"x_raw\"]\n",
        "for m in media_names:\n",
        "    med = np.median(x_df.loc[(x_df.medium==m) & (x_df.x_raw>0), \"x_raw\"])\n",
        "    x_df.loc[x_df.medium==m, \"x_scaled\"] = x_df.loc[x_df.medium==m, \"x_raw\"] / (med + 1e-12)\n",
        "\n",
        "x_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Ground truth: productividad intra-medio y generación de contribuciones incrementales del MMM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "lambda_true = {}\n",
        "for m in media_names:\n",
        "    lam = rng.lognormal(mean=0.0, sigma=0.6, size=K_per_m)\n",
        "    lam = lam / lam.mean()\n",
        "    camps = campaign_df.loc[campaign_df.medium==m, \"campaign\"].tolist()\n",
        "    for i, k in enumerate(camps):\n",
        "        lambda_true[(m, k)] = float(lam[i])\n",
        "\n",
        "campaign_df[\"lambda_true\"] = campaign_df.apply(lambda r: lambda_true[(r[\"medium\"], r[\"campaign\"])], axis=1)\n",
        "\n",
        "# Effective exposure per (medium,campaign,geo) along time\n",
        "s_rows = []\n",
        "for (m,k,g), grp in x_df.groupby([\"medium\",\"campaign\",\"geo\"], sort=False):\n",
        "    grp = grp.sort_values(\"t\")\n",
        "    x = grp[\"x_scaled\"].to_numpy()\n",
        "    a, s = effective_exposure(x, theta=theta_m[m], alpha=alpha_m[m], gamma=gamma_m[m])\n",
        "    out = grp[[\"medium\",\"campaign\",\"geo\",\"t\"]].copy()\n",
        "    out[\"adstock\"] = a\n",
        "    out[\"s_eff\"] = s\n",
        "    s_rows.append(out)\n",
        "s_df = pd.concat(s_rows, ignore_index=True)\n",
        "\n",
        "beta_m = {\"Search\": 120.0, \"Social\": 80.0}\n",
        "\n",
        "sig = s_df.merge(campaign_df[[\"medium\",\"campaign\",\"lambda_true\"]], on=[\"medium\",\"campaign\"], how=\"left\")\n",
        "sig[\"mu\"] = sig[\"lambda_true\"] * sig[\"s_eff\"]\n",
        "\n",
        "C_rows = []\n",
        "for (m,t,g), grp in sig.groupby([\"medium\",\"t\",\"geo\"], sort=False):\n",
        "    total_mu = grp[\"mu\"].sum()\n",
        "    mean = beta_m[m] * total_mu\n",
        "    shape = 20.0\n",
        "    scale = (mean / shape) if mean > 0 else 0.0\n",
        "    C = float(rng.gamma(shape=shape, scale=scale)) if mean > 0 else 0.0\n",
        "    C_rows.append({\"medium\": m, \"t\": int(t), \"geo\": g, \"C_inc\": C})\n",
        "C_df = pd.DataFrame(C_rows)\n",
        "\n",
        "sig2 = sig.merge(C_df, on=[\"medium\",\"t\",\"geo\"], how=\"left\")\n",
        "sig2[\"share_true\"] = sig2[\"mu\"] / (sig2.groupby([\"medium\",\"t\",\"geo\"])[\"mu\"].transform(\"sum\") + 1e-12)\n",
        "sig2[\"z_true\"] = sig2[\"C_inc\"] * sig2[\"share_true\"]\n",
        "\n",
        "sig2.head(), C_df.head(), campaign_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) EM para desagregar C_inc a campañas (sin covariables): ecuaciones cerradas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def em_mmm_to_mta_single_medium(sig_m, C_m, max_iter=200, tol=1e-8, verbose=True):\n",
        "    df = sig_m.merge(C_m, on=[\"t\",\"geo\"], how=\"left\")\n",
        "    df[\"C_inc\"] = df[\"C_inc\"].fillna(0.0)\n",
        "\n",
        "    campaigns = df[\"campaign\"].unique().tolist()\n",
        "    K = len(campaigns)\n",
        "    camp_to_idx = {c:i for i,c in enumerate(campaigns)}\n",
        "\n",
        "    k_idx = df[\"campaign\"].map(camp_to_idx).to_numpy().astype(int)\n",
        "    s = df[\"s_eff\"].to_numpy().astype(float)\n",
        "\n",
        "    tg = pd.factorize(list(zip(df[\"t\"], df[\"geo\"])))[0].astype(int)\n",
        "    C = df[\"C_inc\"].to_numpy().astype(float)\n",
        "\n",
        "    n_groups = int(tg.max()) + 1\n",
        "\n",
        "    lambda_hat = np.ones(K, dtype=float)\n",
        "    history = {\"rel_change\": []}\n",
        "\n",
        "    s_sum_k = np.bincount(k_idx, weights=s, minlength=K) + 1e-12\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        num = lambda_hat[k_idx] * s\n",
        "        den_g = np.bincount(tg, weights=num, minlength=n_groups) + 1e-12\n",
        "        share = num / den_g[tg]\n",
        "        zhat = C * share\n",
        "\n",
        "        z_sum_k = np.bincount(k_idx, weights=zhat, minlength=K)\n",
        "        lambda_new = z_sum_k / s_sum_k\n",
        "\n",
        "        # normalize scale within medium (optional)\n",
        "        lambda_new = lambda_new / (lambda_new.mean() + 1e-12)\n",
        "\n",
        "        rel = float(np.linalg.norm(lambda_new - lambda_hat) / (np.linalg.norm(lambda_hat) + 1e-12))\n",
        "        history[\"rel_change\"].append(rel)\n",
        "\n",
        "        if verbose and (it < 5 or it % 20 == 0):\n",
        "            print(f\"iter {it:3d} | rel_change={rel: .3e}\")\n",
        "\n",
        "        lambda_hat = lambda_new\n",
        "        if rel < tol:\n",
        "            break\n",
        "\n",
        "    df_out = df[[\"campaign\",\"t\",\"geo\",\"s_eff\",\"C_inc\"]].copy()\n",
        "    df_out[\"zhat\"] = zhat\n",
        "    return pd.Series(lambda_hat, index=campaigns, name=\"lambda_hat\"), df_out, history\n",
        "\n",
        "results = []\n",
        "lambda_hats = []\n",
        "histories = {}\n",
        "\n",
        "for m in media_names:\n",
        "    sig_m = sig2.loc[sig2.medium==m, [\"campaign\",\"t\",\"geo\",\"s_eff\"]].copy()\n",
        "    C_m = C_df.loc[C_df.medium==m, [\"t\",\"geo\",\"C_inc\"]].copy()\n",
        "\n",
        "    lam_hat, df_hat, hist = em_mmm_to_mta_single_medium(sig_m, C_m, verbose=False)\n",
        "    df_hat[\"medium\"] = m\n",
        "    results.append(df_hat)\n",
        "\n",
        "    lam_hat_df = lam_hat.reset_index().rename(columns={\"index\":\"campaign\"})\n",
        "    lam_hat_df[\"medium\"] = m\n",
        "    lambda_hats.append(lam_hat_df)\n",
        "\n",
        "    histories[m] = hist\n",
        "\n",
        "zhat_df = pd.concat(results, ignore_index=True)\n",
        "lambda_hat_df = pd.concat(lambda_hats, ignore_index=True)\n",
        "\n",
        "lambda_hat_df.head(), zhat_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Checks y evaluación (ground truth disponible en mock)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "chk = (zhat_df.groupby([\"medium\",\"t\",\"geo\"])[\"zhat\"].sum()\n",
        "       .reset_index()\n",
        "       .merge(C_df, on=[\"medium\",\"t\",\"geo\"], how=\"left\"))\n",
        "chk[\"gap\"] = chk[\"zhat\"] - chk[\"C_inc\"]\n",
        "print(\"max |gap| =\", float(chk[\"gap\"].abs().max()))\n",
        "\n",
        "lam_true_df = campaign_df[[\"medium\",\"campaign\",\"lambda_true\"]].copy()\n",
        "lam_cmp = lam_true_df.merge(lambda_hat_df, on=[\"medium\",\"campaign\"], how=\"left\")\n",
        "\n",
        "corr_lam = lam_cmp.groupby(\"medium\").apply(lambda d: np.corrcoef(d[\"lambda_true\"], d[\"lambda_hat\"])[0,1])\n",
        "print(\"corr(lambda_true, lambda_hat) por medio:\")\n",
        "display(corr_lam)\n",
        "\n",
        "lam_cmp.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "z_cmp = (sig2[[\"medium\",\"campaign\",\"t\",\"geo\",\"z_true\"]]\n",
        "         .merge(zhat_df[[\"medium\",\"campaign\",\"t\",\"geo\",\"zhat\"]], on=[\"medium\",\"campaign\",\"t\",\"geo\"], how=\"left\"))\n",
        "\n",
        "for m in media_names:\n",
        "    d = z_cmp.loc[z_cmp.medium==m]\n",
        "    corr = np.corrcoef(d[\"z_true\"], d[\"zhat\"])[0,1]\n",
        "    rmse = float(np.sqrt(np.mean((d[\"z_true\"] - d[\"zhat\"])**2)))\n",
        "    print(m, \"| corr(z_true, zhat) =\", corr, \"| rmse =\", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Dónde entra adstock + saturación (resumen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "En este enfoque, adstock + saturación se usan para construir la exposición efectiva:\n",
        "\n",
        "\\\\[\n",
        "w_{m,k,t,g} := s_{m,k,t,g} = \\\\text{Hill}(\\\\text{Adstock}(x_{m,k,t,g}))\n",
        "\\\\]\n",
        "\n",
        "y luego el E-step reparte:\n",
        "\n",
        "\\\\[\n",
        "\\\\hat z_{m,k,t,g} = C_{m,t,g}^{inc} \\\\cdot\n",
        "\\\\frac{\\\\lambda_{m,k} w_{m,k,t,g}}\n",
        "{\\\\sum_{k'} \\\\lambda_{m,k'} w_{m,k',t,g}}\n",
        "\\\\]\n",
        "\n",
        "Esto respeta la estructura MMM y conserva masa: \\\\(\\\\sum_k \\\\hat z = C\\\\).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) (Opcional) Extensión a features por campaña (lambda = exp(X beta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Si quieres generalizar / regularizar:\n",
        "\n",
        "\\\\[\n",
        "\\\\lambda_{m,k} = \\\\exp(X_{m,k} \\\\beta_m)\n",
        "\\\\]\n",
        "\n",
        "El M-step pasa a ser un GLM (Poisson/Gamma) con offset (igual que HCP–Brick).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Granularidad horaria (para más adelante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Si el MMM está a nivel semanal/diario pero necesitas atribución horaria:\n",
        "\n",
        "1) Ejecutas EM a nivel (t,g) y obtienes \\\\(\\\\hat z_{m,k,t,g}\\\\)\n",
        "2) Downscaling horario con perfil de delivery y conservación de masa:\n",
        "\n",
        "\\\\[\n",
        "\\\\hat z_{m,k,t,g,h} = \\\\hat z_{m,k,t,g} \\\\cdot\n",
        "\\\\frac{u_{m,k,t,g,h}}{\\\\sum_h u_{m,k,t,g,h}}\n",
        "\\\\]\n",
        "\n",
        "donde \\\\(u\\\\) puede ser impresiones horarias o un modelo de intensidad suavizado.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}