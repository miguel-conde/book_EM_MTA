# Modelo probabilÃ­stico base (sin covariables)

## 1ï¸âƒ£ NotaciÃ³n formal (dejamos esto fijo para todo el curso)

### Ãndices

-   ( h = 1,\\dots,H ) â†’ HCPs

-   ( b = 1,\\dots,B ) â†’ Bricks

### Conjuntos

-   ( \\mathcal{H}\_b \\subset {1,\\dots,H} ): HCPs que pertenecen al brick ( b )

### Variables observadas

-   ( y_b \\in \\mathbb{R}\^+ ): ventas totales del brick ( b )

-   ( w\_{h,b} \\ge 0 ): exposiciÃ³n del HCP ( h ) en el brick ( b )\
    (por ahora puede ser 1 si hay pertenencia)

### ParÃ¡metros (a estimar)

-   ( \\theta_h ): productividad latente del HCP ( h )

## 2ï¸âƒ£ Variable latente clave

Definimos:

\[\
z\_{h,b} \\equiv \\text{ventas atribuibles al HCP } h \\text{ en el brick } b\
\]

con la **restricciÃ³n estructural**:

\[\
\\sum\_{h \\in \\mathcal{H}*b} z*{h,b} = y_b\
\]

ğŸ“Œ Esta restricciÃ³n es lo que hace el problema **no trivial**.

## 3ï¸âƒ£ Caso A â€” Ventas en **unidades** (Poisson)

### ğŸ”¹ Supuesto generativo

Cada HCP genera ventas de forma independiente:

\[\
z\_{h,b} \\sim \\text{Poisson}(\\lambda_h , w\_{h,b})\
\]

donde:

-   ( \\lambda_h \> 0 ): tasa base del HCP

-   ( w\_{h,b} ): exposiciÃ³n en el brick

### ğŸ”¹ Propiedad clave (aditividad)

La suma de Poisson independientes es Poisson:

\[\
y_b = \\sum\_{h \\in b} z\_{h,b}\
\\quad \\Rightarrow \\quad\
y_b \\sim \\text{Poisson}\
\\left(\
\\sum\_{h \\in b} \\lambda_h w\_{h,b}\
\\right)\
\]

ğŸ“Œ Esta propiedad **justifica EM** y simplifica enormemente el E-step.

### ğŸ”¹ Likelihood marginal (observada)

# \[ \\mathcal{L}\_{\\text{obs}}(\\lambda)

\\prod\_{b=1}\^B\
\\text{Poisson}\
\\left(\
y_b ;\\middle\|;\
\\sum\_{h \\in b} \\lambda_h w\_{h,b}\
\\right)\
\]

### ğŸ”¹ Likelihood completa (con latentes)

# \[ \\mathcal{L}\_{\\text{comp}}(\\lambda)

\\prod\_{b=1}\^B\
\\prod\_{h \\in b}\
\\text{Poisson}\
\\left(\
z\_{h,b} \\mid \\lambda_h w\_{h,b}\
\\right)\
\]

ğŸ“Œ EM optimiza la **esperanza del log** de esta expresiÃ³n.

## 4ï¸âƒ£ Caso B â€” Ventas en **euros** (continuas)

AquÃ­ ya no podemos usar Poisson. Dos opciones realistas:

### ğŸ”¹ OpciÃ³n 1: **Gamma** (mi favorita en este contexto)

#### Supuesto

\[\
z\_{h,b} \\sim \\text{Gamma}(k, \\theta_h w\_{h,b})\
\]

-   Media: ( \\mathbb{E}\[z\_{h,b}\] = k \\theta_h w\_{h,b} )

-   Varianza proporcional a la media

-   Soporta asimetrÃ­a

ğŸ“Œ Muy usada en revenue modeling.

#### Suma

Si todos los ( k ) son iguales:

\[\
y_b = \\sum\_{h \\in b} z\_{h,b}\
\\sim\
\\text{Gamma}\
\\left(\
k\|\\mathcal{H}*b\|,\
\\sum*{h \\in b} \\theta_h w\_{h,b}\
\\right)\
\]

### ğŸ”¹ OpciÃ³n 2: **Lognormal** (mÃ¡s frÃ¡gil)

\[\
\\log z\_{h,b} \\sim \\mathcal{N}(\\mu_h + \\log w\_{h,b}, \\sigma\^2)\
\]

Problemas:

-   La suma no es lognormal

-   EM requiere aproximaciones

-   Sensible a outliers

ğŸ“Œ Ãštil si hay colas muy pesadas, pero mÃ¡s compleja.

## 5ï¸âƒ£ Likelihood completa vs marginal (el corazÃ³n de EM)

### ğŸ”¹ Likelihood completa

La que *queremos* maximizar:

# \[ \\log p({y_b, z\_{h,b}} \\mid \\theta)

\\sum\_{b,h}\
\\log p(z\_{h,b} \\mid \\theta_h)\
\]

pero **no podemos** porque ( z\_{h,b} ) no se observa.

### ğŸ”¹ Likelihood marginal (observada)

# \[ \\log p({y_b} \\mid \\theta)

\\sum_b\
\\log\
\\int\
\\prod\_{h \\in b}\
p(z\_{h,b} \\mid \\theta_h)\
,\
\\delta\\Big(\\sum z - y_b\\Big)\
dz\
\]

ğŸ“Œ Integral intratable â†’ EM.

## 6ï¸âƒ£ QuÃ© harÃ¡ EM exactamente (preview)

-   **E-step**\
    Calcula:\
    \[\
    \\mathbb{E}\[z\_{h,b} \\mid y_b, \\theta\^{(t)}\]\
    \]

-   <div>

    # **M-step** Actualiza: \[ \\theta_h\^{(t+1)}

    \\arg\\max\_{\\theta_h}\
    \\sum_b\
    \\mathbb{E}\[z\_{h,b}\]\
    \\log p(z\_{h,b} \\mid \\theta_h)\
    \]

    </div>

## 7ï¸âƒ£ âœ”ï¸ SALIDA â€” Likelihood formal del problema

### ğŸ”¹ Unidades (Poisson)

\[\
\\boxed{\
y_b \\sim \\text{Poisson}\
\\left(\
\\sum\_{h \\in b} \\lambda_h w\_{h,b}\
\\right)\
}\
\]

con latentes:\
\[\
z\_{h,b} \\sim \\text{Poisson}(\\lambda_h w\_{h,b})\
\]

### ğŸ”¹ Euros (Gamma)

\[\
\\boxed{\
y_b \\sim \\text{Gamma}\
\\left(\
k\|\\mathcal{H}*b\|,\
\\sum*{h \\in b} \\theta_h w\_{h,b}\
\\right)\
}\
\]

------------------------------------------------------------------------

Cuando has dicho "Si conociÃ©ramos ğ‘§ â„ , ğ‘ z h,b â€‹

:

PodrÃ­amos estimar fÃ¡cilmente productividad de HCPs" y "Si conociÃ©ramos la productividad de HCPs:" -\> quÃ© estÃ¡s entendiendo como productividad?

------------------------------------------------------------------------

Excelente pregunta â€” y muy importante aclararla ahora, porque **â€œproductividadâ€ puede significar cosas muy distintas** segÃºn el nivel del modelo. Vamos a dejarla **formal, precisa y sin ambigÃ¼edad**.

## ğŸ”‘ QuÃ© significa â€œproductividadâ€ en ESTE curso

Cuando digo **productividad del HCP**, **NO** me refiero a:

-   performance comercial â€œrealâ€

-   causal uplift

-   impacto incremental puro

-   eficiencia del visitador

ğŸ‘‰ Me refiero **estrictamente** a un **parÃ¡metro del modelo generativo**.

## 1ï¸âƒ£ DefiniciÃ³n formal (modelo base sin covariables)

### ğŸ”¹ Caso unidades (Poisson)

La **productividad del HCP ( h )** es:

# \[ \\boxed{ \\lambda_h

\\mathbb{E}\[\\text{unidades generadas por } h \\text{ por unidad de exposiciÃ³n}\]\
}\
\]

y el modelo dice:

\[\
z\_{h,b} \\sim \\text{Poisson}(\\lambda_h , w\_{h,b})\
\]

InterpretaciÃ³n:

-   A igualdad de exposiciÃ³n ( w\_{h,b} )

-   Un HCP con mayor ( \\lambda_h )

-   â€œgeneraâ€ mÃ¡s unidades **en promedio**

ğŸ“Œ Es **una tasa**, no un efecto causal.

### ğŸ”¹ Caso euros (Gamma)

AquÃ­ la productividad es:

# \[ \\boxed{ \\theta_h

\\mathbb{E}\[\\text{â‚¬ generados por } h \\text{ por unidad de exposiciÃ³n}\]\
}\
\]

con:

\[\
z\_{h,b} \\sim \\text{Gamma}(k, \\theta_h w\_{h,b})\
\]

ğŸ“Œ Es el **nivel medio de revenue** atribuible al HCP.

## 2ï¸âƒ£ â€œSi conociÃ©ramos ( z\_{h,b} ), podrÃ­amos estimar productividadâ€

Ahora sÃ­, con precisiÃ³n:

### ğŸ”¹ SupÃ³n que ( z\_{h,b} ) es observado

Entonces para Poisson:

# \[ \\hat{\\lambda}\_h

\\frac{\\sum_b z\_{h,b}}{\\sum_b w\_{h,b}}\
\]

Es el **MLE cerrado** del Poisson.

ğŸ‘‰ No hay EM, no hay dificultad, no hay ambigÃ¼edad.

## 3ï¸âƒ£ â€œSi conociÃ©ramos la productividad, podrÃ­amos repartir ventasâ€

SupÃ³n ahora que conocemos ( \\lambda_h ).

Entonces, para un brick ( b ):

# \[ \\mathbb{E}\[z\_{h,b} \\mid y_b\]

y_b\
\\cdot\
\\frac{\\lambda_h w\_{h,b}}\
{\\sum\_{h' \\in b} \\lambda\_{h'} w\_{h',b}}\
\]

ğŸ“Œ Esta es la **regla de atribuciÃ³n** del E-step.

InterpretaciÃ³n:

-   No decimos â€œel HCP h vendiÃ³ exactamente Xâ€

-   Decimos â€œdado su nivel esperado, le corresponde esta fracciÃ³nâ€

## 4ï¸âƒ£ Importante: lo que *NO* es productividad

Muy importante para evitar malinterpretaciones posteriores:

âŒ No es:

-   efecto incremental causal

-   lift por visita

-   ROI del visitador

-   impacto neto frente a control

âœ”ï¸ SÃ­ es:

-   un **parÃ¡metro latente**

-   coherente con los datos agregados

-   interpretable *dentro del modelo*

Si maÃ±ana cambiamos el modelo (por ejemplo, jerÃ¡rquico bayesiano), **la productividad cambia de significado**.

## 5ï¸âƒ£ Cuando aÃ±adamos covariables (preview)

En el **CapÃ­tulo 5**, redefiniremos:

\[\
\\lambda_h = \\exp(X_h \\beta)\
\]

AquÃ­ la productividad:

-   ya no es un parÃ¡metro libre

-   es una **funciÃ³n de features**

-   permite generalizar a HCPs nuevos

Pero la interpretaciÃ³n base **no cambia**:

> â€œnivel esperado de generaciÃ³n de ventas por exposiciÃ³nâ€

## âœ… Checkpoint conceptual

Dime si esta definiciÃ³n te parece:

-   âœ”ï¸ clara

-   âœ”ï¸ defendible ante negocio

-   âœ”ï¸ compatible con tu intuiciÃ³n

Si estÃ¡s OK, en el siguiente mensaje arrancamos **CapÃ­tulo 3 â€” EM paso a paso (E-step y M-step explÃ­citos)**, ya con fÃ³rmulas cerradas.

------------------------------------------------------------------------

no he entendido bien. QuÃ© hace credit attribution y en que se diferencia de em?

------------------------------------------------------------------------

Muy bien que pares aquÃ­: esta confusiÃ³n es **normal incluso en equipos senior**, porque *credit attribution* y *EM* se usan muchas veces como si fueran lo mismoâ€¦ y **no lo son**.\
Voy a explicarlo **conceptualmente**, sin matemÃ¡ticas innecesarias.

# ğŸ” Credit Attribution vs EM

### QuÃ© hace cada uno y en quÃ© se diferencian

## 1ï¸âƒ£ Â¿QuÃ© es **Credit Attribution**?

### ğŸ‘‰ Es un **objetivo**, no un algoritmo

**Credit attribution** significa:

> *Asignar una fracciÃ³n de un resultado agregado (venta, revenue, conversiÃ³n) a varios agentes que han participado.*

Ejemplos:

-   Marketing: repartir una conversiÃ³n entre canales

-   Tu caso: repartir ventas del brick entre HCPs

-   Finanzas: repartir P&L entre desks

ğŸ“Œ **No dice cÃ³mo hacerlo**, solo **quÃ© quieres obtener**.

### ğŸ”¹ Resultado tÃ­pico

Algo como:

| Brick | HCP | CrÃ©dito asignado |
|-------|-----|------------------|
| B1    | H1  | 30%              |
| B1    | H2  | 50%              |
| B1    | H3  | 20%              |

Eso **es credit attribution**.

## 2ï¸âƒ£ Â¿QuÃ© NO es credit attribution?

-   âŒ No es necesariamente estadÃ­stico

-   âŒ No implica likelihood

-   âŒ No implica variables latentes

-   âŒ No implica inferencia

Ejemplos de *credit attribution* **no estadÃ­stico**:

-   Repartir ventas proporcionalmente al nÂº de visitas

-   Repartir ventas a partes iguales

-   Reglas de negocio (â€œel mÃ¡s senior se lleva mÃ¡sâ€)

ğŸ‘‰ Todos hacen attribution, **ninguno usa EM**.

## 3ï¸âƒ£ Entonces, Â¿quÃ© es **EM**?

### ğŸ‘‰ EM es un **algoritmo de inferencia estadÃ­stica**

EM responde a:

> *Dado un modelo probabilÃ­stico con variables latentes, Â¿cÃ³mo estimo sus parÃ¡metros?*

EM:

-   Maximiza una **likelihood**

-   Asume un **modelo generativo**

-   Produce estimaciones consistentes (bajo supuestos)

ğŸ“Œ EM **no tiene como objetivo principal repartir crÃ©ditos**,\
eso es una **consecuencia** del E-step.

## 4ï¸âƒ£ RelaciÃ³n exacta entre ambos (la clave)

### ğŸ”‘ En tu problema:

-   **Credit attribution** =\
    â€œÂ¿CuÃ¡nto de la venta del brick corresponde a cada HCP?â€

-   **EM** =\
    â€œÂ¿QuÃ© valores de productividad y contribuciones latentes explican mejor los datos agregados?â€

### ğŸ” ConexiÃ³n

En EM:

-   El **E-step** calcula:\
    \[\
    \\mathbb{E}\[z\_{h,b} \\mid y_b\]\
    \]

-   Eso **es una attribution probabilÃ­stica**

ğŸ‘‰ Pero:

-   La attribution **no es el objetivo**

-   Es un **subproducto necesario** para aprender el modelo

## 5ï¸âƒ£ Diferencia en una tabla clara

| Aspecto             | Credit Attribution | EM              |
|---------------------|--------------------|-----------------|
| QuÃ© es              | Objetivo           | Algoritmo       |
| Define              | QuÃ© se reparte     | CÃ³mo se infiere |
| Requiere modelo     | No                 | SÃ­              |
| Usa likelihood      | No                 | SÃ­              |
| Produce parÃ¡metros  | No                 | SÃ­              |
| Produce attribution | SÃ­                 | Indirectamente  |

## 6ï¸âƒ£ AnalogÃ­a muy clara (mental model)

### ğŸ‚ Repartir una tarta

-   **Credit attribution**:\
    â€œÂ¿CÃ³mo repartimos la tarta entre 5 personas?â€

-   **EM**:\
    â€œAsumimos que cada persona tiene un apetito distinto (latente).\
    Observamos cuÃ¡nto se comiÃ³ el grupo en total.\
    Inferimos los apetitos y, como consecuencia, cuÃ¡nto comiÃ³ cada uno.â€

ğŸ‘‰ EM **explica** el reparto, no lo decide arbitrariamente.

## 7ï¸âƒ£ Resumen en una frase (para que te quede)

> **Credit attribution es el *quÃ©***\
> **EM es el *cÃ³mo*, bajo un modelo estadÃ­stico explÃ­cito**

## âœ… Checkpoint

Dime si ahora ves clara esta separaciÃ³n:

-   Â¿Te queda claro que **EM no es un mÃ©todo de attribution**, sino un mÃ©todo de inferencia?

-   Â¿Ves por quÃ© *muchos* sistemas de attribution **no necesitan EM**, pero el tuyo sÃ­?

Si estÃ¡s OK, en el siguiente paso entramos de lleno en **CapÃ­tulo 3 â€” EM paso a paso**, donde verÃ¡s cÃ³mo la attribution sale *automÃ¡ticamente* del E-step.

------------------------------------------------------------------------

es decir, que EM es una de las posibles maneras de CÃ“MO hacer credit attribution (el QUÃ‰). Otras maneras son mixture models y bayesian inference. Es asÃ­?

------------------------------------------------------------------------

Casi perfecto ğŸ‘ â€” solo hace falta **un pequeÃ±o ajuste conceptual** para que quede *exactamente correcto*.\
Voy a dejarlo cristalino y cerramos esta parte.

## âœ… La forma correcta de pensarlo

### âœ”ï¸ Lo que **sÃ­** es correcto

-   **Credit attribution** es el **QUÃ‰**\
    â†’ repartir un resultado agregado entre agentes

-   Existen **muchas maneras** de hacer credit attribution

### â— El ajuste importante

**Mixture models NO son una alternativa a EM**\
**Bayesian inference NO es una alternativa a EM**

ğŸ‘‰ EstÃ¡n en **niveles distintos del stack conceptual**.

## ğŸ§± Stack correcto (de arriba a abajo)

### 1ï¸âƒ£ **QUÃ‰** â€” Credit attribution

> El problema de negocio

Ejemplo:

-   Repartir ventas del brick entre HCPs

### 2ï¸âƒ£ **MODELO** â€” Mixture / Latent variable model

> CÃ³mo supones que se generan los datos

Ejemplos:

-   Mixture model

-   Latent additive model

-   Hierarchical model

ğŸ“Œ AquÃ­ decides **quÃ© es ( z\_{h,b} )** y cÃ³mo se genera.

### 3ï¸âƒ£ **CÃ“MO INFERIR** â€” EM / Bayesian / VI

> CÃ³mo estimas los parÃ¡metros del modelo

Ejemplos:

-   **EM** â†’ Maximum Likelihood con latentes

-   **Bayesian inference** â†’ posterior completo

-   **Variational inference** â†’ aproximaciÃ³n

## ğŸ“Š DÃ³nde encaja EM exactamente

EM es:

> **Una forma de inferir parÃ¡metros en modelos con variables latentes**

No es:

-   ni el problema

-   ni el modelo

## ğŸ” Reescribiendo tu frase (100% correcta)

Tu frase:

> *â€œEM es una de las posibles maneras de CÃ“MO hacer credit attribution (el QUÃ‰). Otras maneras son mixture models y bayesian inference.â€*

La versiÃ³n **exacta** serÃ­a:

> **â€œCredit attribution es el QUÃ‰.\
> Definimos un modelo latente (por ejemplo, un mixture model).\
> EM es una de las maneras de inferir ese modelo; la inferencia bayesiana es otra.â€**

## ğŸ§  Ejemplo concreto (tu caso)

-   QUÃ‰:\
    Repartir ventas del brick entre HCPs

-   MODELO:\
    ( z\_{h,b} \\sim \\text{Poisson}(\\lambda_h w\_{h,b}) )

-   CÃ“MO (opciÃ³n A):\
    EM â†’ MLE de ( \\lambda_h )

-   CÃ“MO (opciÃ³n B):\
    Bayes â†’ ( p(\\lambda_h \\mid y) )

Ambos producen:

-   una attribution

-   pero con propiedades distintas

## ğŸ“Œ Regla mental Ãºtil (para siempre)

> **Model â‰  Inference â‰  Business objective**

Si confundes uno con otro:

-   los modelos se vuelven â€œreligiÃ³nâ€

-   las discusiones con negocio se rompen