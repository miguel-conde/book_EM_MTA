# Modelo probabilístico base (Poisson) y notación

## Introducción

Este capítulo fija el modelo probabilístico base que usaremos durante el resto del libro. El objetivo es doble: (i) dejar la notación cerrada y estable, y (ii) formalizar el mecanismo “agregado → desagregado” como un modelo generativo con variables latentes.

El propósito operativo de este capítulo es que el lector salga con un modelo completamente especificado y reutilizable: **qué observamos** ($y_b$ y $w_{h,b}$), **qué variable latente** introducimos ($z_{h,b}$) y qué suposiciones probabilísticas ligan ambos niveles. Para ello, primero fijamos la notación que se mantiene en el resto del libro, y después proponemos dos familias de modelos según la naturaleza del agregado (unidades frente a euros).

En unidades, el caso Poisson permite escribir una *likelihood observada* cerrada gracias a la propiedad de aditividad; en euros, discutimos dos alternativas positivas continuas (Gamma y Lognormal), anticipando qué aspectos se vuelven más o menos manejables. Cerramos con una vista previa de qué hace EM (por qué aparece la distinción entre *likelihood completa* y *likelihood marginal*) y con una aclaración conceptual que separa el **objetivo** de attribution del **nivel de modelado** y del **nivel de inferencia**.

## Notación formal (la dejamos fija para todo el libro)

Usaremos dos índices principales. El índice $h=1,\dots,H$ recorre HCPs y el índice $b=1,\dots,B$ recorre bricks. Denotamos por $\mathcal{H}_b\subset\{1,\dots,H\}$ el conjunto de HCPs que pertenecen al brick $b$.

Las variables observadas son $y_b\in\mathbb{R}^+$, las ventas totales del brick $b$, y $w_{h,b}\ge 0$, la exposición (u oportunidad) del HCP $h$ en el brick $b$; por ahora puede ser 1 si hay pertenencia y 0 si no. En algunas secciones también se usa la notación $\omega_{b,h}$ para este mismo término de exposición; véase el [Apéndice 93](93-ap-3-exposure.qmd) para la nomenclatura técnica (exposure/offset) y su interpretación.

Los parámetros del modelo (a estimar) representan productividad latente por HCP. En el capítulo usaremos notación genérica $\theta_h$ y, en el caso Poisson de unidades, escribiremos $\lambda_h$.

## Variable latente clave

Definimos la variable latente

$$
z_{h,b} \equiv \text{ventas atribuibles al HCP } h \text{ en el brick } b
$$

con la restricción estructural

$$
\sum_{h \in \mathcal{H}_b} z_{h,b} = y_b.
$$

Esta restricción es lo que hace el problema no trivial: el **dato observado** está agregado, pero el **objeto de interés** está a nivel individual.

## Caso A — Ventas en unidades (Poisson)

En unidades, un supuesto estándar es que cada HCP genera ventas de forma independiente y que el número de unidades atribuibles a un HCP dentro de un brick sigue una Poisson escalada por la exposición:

$$
z_{h,b} \sim \text{Poisson}(\lambda_h w_{h,b}),
$$

donde $\lambda_h>0$ es la tasa base (productividad) del HCP y $w_{h,b}$ es la exposición en el brick.

La propiedad clave es la aditividad: la suma de Poisson independientes es Poisson. Por tanto,

$$
y_b = \sum_{h \in b} z_{h,b}
\quad \Rightarrow \quad
y_b \sim \text{Poisson}\left(\sum_{h \in b} \lambda_h w_{h,b}\right).
$$

Esta propiedad es la que permite escribir una likelihood observada cerrada y, más adelante, obtener un *E-step* particularmente simple.

La *likelihood marginal (observada)* es:

$$
\mathcal{L}_{\text{obs}}(\lambda)=
\prod_{b=1}^B
\mathrm{Poisson}\left(y_b \,\Big|\, \sum_{h \in b} \lambda_h w_{h,b}\right).
$$

Y la *likelihood completa (con latentes)* es:

$$
\mathcal{L}_{\text{comp}}(\lambda)=
\prod_{b=1}^B
\prod_{h \in b}
\mathrm{Poisson}\left(z_{h,b} \,\big|\, \lambda_h w_{h,b}\right).
$$

EM trabajará con la esperanza del log de esta expresión *completa*.

## Caso B — Ventas en euros (continuas)

Cuando el agregado observado está en euros (variable continua positiva), Poisson deja de ser apropiada. Dos opciones realistas en este contexto son Gamma y Lognormal.

### Opción 1: Gamma

Un supuesto habitual es:

$$
z_{h,b} \sim \text{Gamma}(k, \theta_h w_{h,b}).
$$

En esta parametrización, la media es $\mathbb{E}[z_{h,b}] = k\,\theta_h w_{h,b}$; la varianza es proporcional a la media y el modelo soporta asimetría. Es una elección frecuente en *revenue modeling*.

Si todos los $k$ son iguales, entonces la suma también es Gamma:

$$
y_b = \sum_{h \in b} z_{h,b}
\sim \text{Gamma}\left(k|\mathcal{H}_b|,\, \sum_{h \in b} \theta_h w_{h,b}\right).
$$

### Opción 2: Lognormal

Una alternativa es:

$$
\log z_{h,b} \sim \mathcal{N}(\mu_h + \log w_{h,b}, \sigma^2).
$$

En este caso, la suma no es lognormal, EM suele requerir aproximaciones y el comportamiento puede ser sensible a outliers. Es útil cuando hay colas muy pesadas, a cambio de mayor complejidad.

## Likelihood completa vs marginal (el corazón de EM)

En modelos con variables latentes, conviene distinguir entre la *likelihood completa* y la *observada*.

La *likelihood completa* (la que querríamos maximizar si observásemos $z$) tiene la forma:

$$
\log p(\{y_b, z_{h,b}\} \mid \theta)
= \sum_{b,h} \log p(z_{h,b} \mid \theta_h).
$$

En la práctica no podemos hacerlo porque $z_{h,b}$ no se observa. La *likelihood marginal (observada)* integra los latentes imponiendo la restricción de suma:

$$
\log p(\{y_b\} \mid \theta)
= \sum_b \log
\int
\prod_{h \in b} p(z_{h,b} \mid \theta_h)
\,\delta\Big(\sum_{h \in b} z_{h,b} - y_b\Big)
\,dz.
$$

La presencia de esta integral (con una restricción dura) es lo que vuelve intratable el problema en su forma directa y es precisamente donde EM aporta un procedimiento sistemático.

## Qué hará EM exactamente

EM alterna dos pasos. En el *E-step*, dado un valor actual de los parámetros $\theta^{(t)}$, se calcula:

$$
\mathbb{E}[z_{h,b} \mid y_b, \theta^{(t)}].
$$

En el *M-step*, con $\mathbb{E}[z]$ fijo, se actualizan parámetros maximizando el objetivo correspondiente; en el esquema base, para cada $h$:

Aquí el superíndice $(t)$ indexa iteraciones de EM (y $(t+1)$ denota el valor tras la actualización), pero lo indicaremos solo cuando sea necesario; la mayor parte del texto omite el índice de iteración por legibilidad.

$$
θ_h^{(t+1)}=
\arg\max_{\theta_h}
\sum_b \mathbb{E}[z_{h,b}]\, \log p(z_{h,b} \mid \theta_h).
$$

## Salida — Likelihood formal del problema

Para tener el problema completamente fijado, estas son las expresiones formales que usaremos como referencia.

### Unidades (Poisson)

$$
\boxed{y_b \sim \text{Poisson}\left(\sum_{h \in b} \lambda_h w_{h,b}\right)}
$$

con latentes

$$
z_{h,b} \sim \text{Poisson}(\lambda_h w_{h,b}).
$$

### Euros (Gamma)

$$
\boxed{y_b \sim \text{Gamma}\left(k|\mathcal{H}_b|,\, \sum_{h \in b} \theta_h w_{h,b}\right)}.
$$

## Aclaraciones conceptuales

Antes de entrar en la derivación de EM (Capítulo 3), conviene fijar dos aclaraciones: qué significa “productividad” dentro del modelo y cómo se relaciona el objetivo de *credit attribution* con el modelo probabilístico y el método de inferencia.

### Qué significa “productividad” en este libro

En este libro, “productividad del HCP” no se interpreta como performance comercial “real”, causal uplift, impacto incremental puro, eficiencia del visitador, ni métricas de ROI. Se usa estrictamente como un **parámetro del modelo generativo**, cuya interpretación es interna al modelo.

En particular, se trata de un **parámetro latente** coherente con los datos agregados y utilizable para inducir un reparto dentro del modelo; *no es un efecto causal* ni pretende medir impacto incremental.

En unidades (Poisson), la productividad del HCP $h$ se representa por $\lambda_h$ y se interpreta como una tasa por unidad de exposición:

$$
\boxed{\lambda_h = \mathbb{E}[\text{unidades generadas por } h \text{ por unidad de exposición}]}.
$$

En euros (Gamma), la productividad se representa por $\theta_h$ y se interpreta como un nivel medio (por unidad de exposición):

$$
\boxed{\theta_h = \mathbb{E}[\text{€ generados por } h \text{ por unidad de exposición}]}.
$$

Si $z_{h,b}$ fuese observado, en el caso Poisson el MLE sería cerrado:

$$
\hat{\lambda}_h = \frac{\sum_b z_{h,b}}{\sum_b w_{h,b}}.
$$

Y si $\lambda_h$ fuese conocido, el reparto esperado dentro de un brick se expresa como:

$$
\mathbb{E}[z_{h,b} \mid y_b]
= y_b\cdot
\frac{\lambda_h w_{h,b}}{\sum_{h' \in b} \lambda_{h'} w_{h',b}}.
$$

La lectura correcta de esta expresión es **reparto esperado**: no se afirma que un HCP haya generado exactamente una cantidad concreta, sino una fracción esperada condicionada al agregado y al modelo.

En el Capítulo 5, cuando se incorporan covariables, se reemplaza un $\lambda_h$ libre por una parametrización del tipo $\lambda_h=\exp(X_h\beta)$. La interpretación base no cambia: sigue siendo un nivel esperado por exposición, ahora compartiendo información entre HCPs.

Si se cambia el modelo de forma más profunda (por ejemplo, hacia una formulación jerárquica bayesiana), la interpretación de “productividad” también cambia: el parámetro deja de ser un escalar fijo y pasa a formar parte de una estructura probabilística más rica.

### Credit attribution, modelo e inferencia

Conviene separar tres niveles: **objetivo**, **modelo** e **inferencia**.

En la parte superior está el objetivo: *credit attribution* es el “qué”, es decir, repartir un resultado agregado (venta, revenue, conversión) entre varios agentes. En nuestro caso, repartir ventas del brick entre HCPs.

En un segundo nivel está el **modelo**: cómo se supone que se generan los datos. Un mixture model o, más en general, un modelo con variables latentes especifica qué es $z_{h,b}$ y cómo se relaciona con parámetros como $\lambda_h$ o $\theta_h$.

En un tercer nivel está el **método de inferencia**: cómo se estiman los parámetros del modelo. EM es una forma de inferir parámetros vía máxima verosimilitud en presencia de latentes. La inferencia bayesiana es otra opción, en cuyo caso se infiere un posterior completo.

Una forma de recordarlo es distinguir entre problema, modelo e inferencia: el reparto es el objetivo, el modelo es la hipótesis de generación, y EM es un procedimiento para ajustar ese modelo. En ese sentido, EM no “decide” un reparto arbitrario: lo deriva de parámetros estimados bajo supuestos explícitos.

### Credit attribution: definición y producto esperado

En este libro, *credit attribution* se entiende como el **objetivo** de asignar una fracción de un resultado agregado (venta, revenue, conversión) a varios agentes que han participado. Esta formulación aparece en distintos dominios: repartir una conversión entre canales de marketing, repartir P\&L entre *desks* de trading, o —en nuestro caso— repartir ventas del brick entre HCPs.

De forma operativa, el resultado final de un proceso de attribution se representa como un conjunto de asignaciones por brick, por ejemplo:

| Brick | HCP | Crédito asignado |
|---|---|---|
| B1 | H1 | 30% |
| B1 | H2 | 50% |
| B1 | H3 | 20% |

### Qué no exige credit attribution

El objetivo de attribution, por sí mismo, no impone una forma concreta de cálculo. En particular, puede existir attribution sin que haya un modelo probabilístico explícito, sin likelihood, sin variables latentes y sin un método de inferencia estadística asociado. En la práctica, esto incluye reglas no estadísticas (por ejemplo, reparto proporcional al número de visitas, reparto a partes iguales o reglas de negocio).

### Qué es EM en este contexto

EM es un algoritmo de **inferencia estadística**: dado un modelo probabilístico con variables latentes, proporciona un procedimiento para estimar parámetros por máxima verosimilitud.

En términos operativos:

- maximiza una likelihood;
- asume un modelo generativo;
- produce estimaciones consistentes bajo los supuestos del modelo.

Su salida primaria son **parámetros** (por ejemplo, $\lambda_h$ o $\theta_h$); el reparto esperado aparece como parte del *E-step*.

En el caso extremo donde $z_{h,b}$ fuese observado, el problema se reduce a estimación directa (por ejemplo, el MLE cerrado del caso Poisson) y no hay necesidad de EM.

### Relación exacta entre attribution y EM

En este problema, puede formularse la diferencia de manera precisa.

- En términos de objetivo, attribution pregunta: cuánto de la venta del brick corresponde a cada HCP.
- En términos de inferencia, EM pregunta: qué valores de productividad y contribuciones latentes explican mejor los datos agregados bajo el modelo.

Ambos quedan conectados por el *E-step*, que calcula cantidades del tipo $\mathbb{E}[z_{h,b} \mid y_b]$. Esta expectativa es una *attribution probabilística*: no afirma un valor exacto, sino un **reparto esperado** condicionado al agregado y a los parámetros.

Una forma compacta de visualizar la diferencia es la siguiente:

| Aspecto | Credit attribution | EM |
|---|---|---|
| Qué es | Objetivo | Algoritmo |
| Define | Qué se reparte | Cómo se infiere |
| Requiere modelo | No necesariamente | Sí |
| Usa likelihood | No necesariamente | Sí |
| Produce parámetros | No | Sí |
| Produce attribution | Sí | Como consecuencia del *E-step* |

### Tres niveles: objetivo, modelo e inferencia

Para evitar confusiones recurrentes, conviene fijar explícitamente tres niveles.

En el nivel superior está el objetivo (credit attribution). En el nivel intermedio está el modelo (por ejemplo, mixture models, modelos aditivos latentes o modelos jerárquicos), donde se decide qué representa $z_{h,b}$ y cómo se genera. En el nivel inferior está el mecanismo de inferencia (por ejemplo, EM, inferencia bayesiana o inferencia variacional), que especifica cómo se ajusta el modelo.

En esta terminología, un mixture model no es una alternativa a EM: pertenece al nivel de modelado. La inferencia bayesiana no es una alternativa al modelo: pertenece al nivel de inferencia. Una formulación equivalente, útil como regla de trabajo, es:

> **Modelo** $\ne$ **Inferencia** $\ne$ **Objetivo** de negocio.

Una analogía simple es el reparto de una tarta: el objetivo es repartir; el modelo postula preferencias latentes; se observa solo el total consumido; EM ajusta esas preferencias a partir del total observado y, como consecuencia, induce un reparto esperado.

En este marco, la frase correcta es: credit attribution es el objetivo; se define un modelo latente; EM es una de las maneras de inferir ese modelo, y la inferencia bayesiana es otra. La inferencia variacional es una aproximación adicional cuando el posterior o la likelihood marginal no se manejan de forma cerrada.

Un esquema mínimo, equivalente al que aparece en la discusión original, es:

- Objetivo: repartir ventas del brick entre HCPs.
- Modelo: $z_{h,b} \sim \text{Poisson}(\lambda_h w_{h,b})$ (u otra elección latente equivalente).
- Inferencia (opción A): EM para obtener MLE de $\lambda_h$.
- Inferencia (opción B): Bayes para obtener, por ejemplo, $p(\lambda_h \mid y)$.

Todas estas elecciones producen una attribution, pero con propiedades distintas.

Si se mezclan estos niveles, el debate tiende a desplazarse del objetivo a discusiones doctrinales sobre el modelo o el método, y la comunicación con negocio se deteriora.

## Resumen

- El **contrato del modelo base** es: observamos $y_b$ (agregado por brick) y modelamos una descomposición *latente* $z_{h,b}$ con $\sum_{h\in\mathcal{H}_b} z_{h,b}=y_b$; la exposición $w_{h,b}$ escala el aporte esperado de cada HCP.
- En unidades, $z_{h,b}\sim\text{Poisson}(\lambda_h w_{h,b})$ induce una distribución cerrada para el agregado y facilita distinguir *likelihood observada (marginal)* y *likelihood completa*.
- En euros, Gamma y Lognormal son alternativas positivas continuas; Gamma suele ser más manejable cuando se quiere conservar **aditividad** (bajo supuestos adecuados sobre el parámetro de forma).
- La idea puente al Capítulo 3 es que EM trabaja sobre la *likelihood completa* y utiliza cantidades del tipo $\mathbb{E}[z_{h,b}\mid y_b]$, que se leen como un **reparto esperado** compatible con el modelo.